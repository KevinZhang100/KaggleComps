{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import warnings\n","warnings.simplefilter(\"ignore\")\n","\n","import numpy as np\n","import pandas as pd\n","import collections\n","import os\n","import re\n","import math\n","import copy\n","import torch\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import f1_score\n","\n","train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n","test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n","print(train.columns)\n","\n","# gpu acceleration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#misclassifed datapoints in train\n","train.loc[train['text'] == 'like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit', 'target'] = 0\n","train.loc[train['text'] == 'Hellfire is surrounded by desires so be careful and donÛªt let your desires control you! #Afterlife', 'target'] = 0\n","train.loc[train['text'] == 'To fight bioterrorism sir.', 'target'] = 0\n","train.loc[train['text'] == '.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4', 'target'] = 1\n","train.loc[train['text'] == 'CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97/Georgia Ave Silver Spring', 'target'] = 1\n","train.loc[train['text'] == '#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption', 'target'] = 0\n","train.loc[train['text'] == 'In #islam saving a person is equal in reward to saving all humans! Islam is the opposite of terrorism!', 'target'] = 0\n","train.loc[train['text'] == 'Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE', 'target'] = 1\n","train.loc[train['text'] == 'RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http://t.co/JlzK2HdeTG', 'target'] = 1\n","train.loc[train['text'] == \"Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...\", 'target'] = 0\n","train.loc[train['text'] == \"wowo--=== 12000 Nigerian refugees repatriated from Cameroon\", 'target'] = 0\n","train.loc[train['text'] == \"He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\", 'target'] = 0\n","train.loc[train['text'] == \"Hellfire! We donÛªt even want to think about it or mention it so letÛªs not do anything that leads to it #islam!\", 'target'] = 0\n","train.loc[train['text'] == \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\", 'target'] = 0\n","train.loc[train['text'] == \"Caution: breathing may be hazardous to your health.\", 'target'] = 1\n","train.loc[train['text'] == \"I Pledge Allegiance To The P.O.P.E. And The Burning Buildings of Epic City. ??????\", 'target'] = 0\n","train.loc[train['text'] == \"#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect\", 'target'] = 0\n","train.loc[train['text'] == \"that horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time\", 'target'] = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def clean(tweet): \n","    # Special characters\n","    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n","    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n","    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n","    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n","    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n","    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n","    tweet = re.sub(r\"let\\x89Ûªs\", \"let's\", tweet)\n","    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n","    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n","    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n","    tweet = re.sub(r\"å_\", \"\", tweet)\n","    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n","    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n","    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n","    tweet = re.sub(r\"åÊ\", \"\", tweet)\n","    tweet = re.sub(r\"åÈ\", \"\", tweet)\n","    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)    \n","    tweet = re.sub(r\"Ì©\", \"e\", tweet)\n","    tweet = re.sub(r\"å¨\", \"\", tweet)\n","    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n","    tweet = re.sub(r\"åÇ\", \"\", tweet)\n","    tweet = re.sub(r\"å£3million\", \"3 million\", tweet)\n","    tweet = re.sub(r\"åÀ\", \"\", tweet)\n","    \n","    # Character entity references\n","    tweet = re.sub(r\"&gt;\", \">\", tweet)\n","    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n","    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n","    \n","    # Typos, slang, and informal abbreviations\n","    tweet = re.sub(r\"w/e\", \"whatever\", tweet)\n","    tweet = re.sub(r\"w/\", \"with\", tweet)\n","    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n","    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n","    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n","    tweet = re.sub(r\"amirite\", \"am I right\", tweet)\n","    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n","    tweet = re.sub(r\"<3\", \"love\", tweet)\n","    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n","    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n","    tweet = re.sub(r\"8/5/2015\", \"2015-08-05\", tweet)\n","    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n","    tweet = re.sub(r\"8/6/2015\", \"2015-08-06\", tweet)\n","    tweet = re.sub(r\"10:38PM\", \"10:38 PM\", tweet)\n","    tweet = re.sub(r\"10:30pm\", \"10:30 PM\", tweet)\n","    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n","    tweet = re.sub(r\"lmao\", \"laughing my ass off\", tweet)   \n","    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\n","    \n","    # URLs\n","    tweet = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", tweet)\n","    \n","    # Words with punctuations and special characters\n","    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\"\n","    for p in punctuations:\n","        tweet = tweet.replace(p, f' {p} ')\n","        \n","    # ... and ..\n","    tweet = tweet.replace('...', ' ... ')\n","    if '...' not in tweet:\n","        tweet = tweet.replace('..', ' ... ')      \n","        \n","    # Acronyms\n","    tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", tweet)\n","    tweet = re.sub(r\"mÌ¼sica\", \"music\", tweet)\n","    tweet = re.sub(r\"okwx\", \"Oklahoma City Weather\", tweet)\n","    tweet = re.sub(r\"arwx\", \"Arkansas Weather\", tweet)    \n","    tweet = re.sub(r\"gawx\", \"Georgia Weather\", tweet)  \n","    tweet = re.sub(r\"scwx\", \"South Carolina Weather\", tweet)  \n","    tweet = re.sub(r\"cawx\", \"California Weather\", tweet)\n","    tweet = re.sub(r\"tnwx\", \"Tennessee Weather\", tweet)\n","    tweet = re.sub(r\"azwx\", \"Arizona Weather\", tweet)  \n","    tweet = re.sub(r\"alwx\", \"Alabama Weather\", tweet)\n","    tweet = re.sub(r\"wordpressdotcom\", \"wordpress\", tweet)    \n","    tweet = re.sub(r\"usNWSgov\", \"United States National Weather Service\", tweet)\n","    tweet = re.sub(r\"Suruc\", \"Sanliurfa\", tweet)\n","    \n","    return tweet\n","\n","\n","train['text'] = train['text'].apply(clean)\n","test['text'] = test['text'].apply(clean)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","#get top most important words\n","tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n","matrix_train = tfidf_vectorizer.fit_transform(train['text'])\n","words = tfidf_vectorizer.get_feature_names_out()\n","\n","for i, row in enumerate(matrix_train):\n","    row = row.toarray().flatten()\n","    curr_top = row.argsort()[-2:]\n","    train['text'].iloc[i] = words[curr_top[1]] + ' [SEP] ' + words[curr_top[0]] + ' [SEP] ' + train['text'].iloc[i] \n","    \n","matrix_test = tfidf_vectorizer.transform(test['text'])\n","\n","for i, row in enumerate(matrix_test):\n","    row = row.toarray().flatten()\n","    curr_top = row.argsort()[-2:]\n","    test['text'].iloc[i] = words[curr_top[1]] + ' [SEP] ' + words[curr_top[0]] + ' [SEP] ' + test['text'].iloc[i] "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train['keyword'].fillna('None', inplace=True)\n","train['location'].fillna('None', inplace=True)\n","train['text'] = train['keyword'] + ' [SEP] ' + train['location']  + ' [SEP] ' + train['text']\n","\n","test['keyword'].fillna('None', inplace=True)\n","test['location'].fillna('None', inplace=True)\n","test['text'] = test['keyword'] + ' [SEP] ' + test['location']  + ' [SEP] ' + test['text']\n","print(test['text'].iloc[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","X = train['text']\n","y = train['target']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.02, random_state=42)\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n","\n","train_encodings = tokenizer(list(X_train), padding=True, return_tensors=\"pt\")\n","test_encodings = tokenizer(list(X_test), padding=True, return_tensors=\"pt\")\n","print(train_encodings['input_ids'].shape)\n","print(test_encodings['input_ids'].shape)\n","\n","train_labels = torch.tensor(y_train.values).to(device)\n","test_labels = torch.tensor(y_test.values).to(device)\n","\n","#attention mask for padding\n","train_dataset = TensorDataset(train_encodings['input_ids'].to(device), train_encodings['attention_mask'].to(device), train_labels)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","\n","test_dataset = TensorDataset(test_encodings['input_ids'].to(device), test_encodings['attention_mask'].to(device), test_labels)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-base\", num_labels=2).to(device)\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","\n","epochs = 3\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=(len(train_loader) * epochs))\n","\n","model.train()\n","models = []\n","\n","for i in range(epochs):\n","  tot_loss = 0\n","  for batch in train_loader:\n","    optimizer.zero_grad() #need to use it to reset gradient\n","    inputs, mask, labels = [b.to(device) for b in batch]\n","    outputs = model(inputs, attention_mask=mask, labels=labels)\n","    loss = outputs.loss\n","    loss.backward()\n","    optimizer.step() #updates model\n","    scheduler.step()\n","    tot_loss += loss.item()\n","\n","  models.append(copy.deepcopy(model.state_dict()))\n","  print(f\"Epoch {i} loss: {tot_loss}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","for epoch, state_dict in enumerate(models):\n","\n","  model.load_state_dict(state_dict)\n","  model.eval()\n","  pred = []\n","  true = []\n","\n","  for batch in test_loader:\n","    inputs, mask, labels = [b.to(device) for b in batch]\n","    outputs = model(inputs, attention_mask=mask)\n","    logits = outputs.logits\n","    pred.extend(torch.argmax(logits, dim=1).tolist())\n","    true.extend(labels.tolist())\n","\n","  accuracy = accuracy_score(true, pred)\n","  precision = precision_score(true, pred)\n","  recall = recall_score(true, pred)\n","  f1 = f1_score(true, pred)\n","\n","  print(f\"Model from Epoch {epoch+1}:\")\n","  print(f\"Accuracy: {accuracy}\")\n","  print(f\"Precision: {precision}\")\n","  print(f\"Recall: {recall}\")\n","  print(f\"F1 Score: {f1}\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.load_state_dict(models[1])\n","\n","submit_encodings = tokenizer(list(test['text']), padding=True, return_tensors=\"pt\")\n","submit_ids = torch.tensor(test['id'].values).to(device)\n","\n","submit_dataset = TensorDataset(submit_encodings['input_ids'].to(device), submit_encodings['attention_mask'].to(device), submit_ids)\n","submit_loader = DataLoader(submit_dataset, batch_size=32)\n","\n","sub = []\n","sub_ids = []\n","\n","for batch in submit_loader:\n","    inputs, mask, ids = [b.to(device) for b in batch]\n","    outputs = model(inputs, attention_mask=mask)\n","    logits = outputs.logits\n","    sub.extend(torch.argmax(logits, dim=1).tolist())\n","    sub_ids.extend(ids.tolist())\n","\n","res = pd.DataFrame({'id': sub_ids,'target':sub})\n","res.to_csv(\"submission.csv\", index=False)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":869809,"sourceId":17777,"sourceType":"competition"},{"datasetId":2134,"sourceId":3586,"sourceType":"datasetVersion"},{"datasetId":459672,"sourceId":865552,"sourceType":"datasetVersion"},{"datasetId":493784,"sourceId":918868,"sourceType":"datasetVersion"}],"dockerImageVersionId":30648,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
